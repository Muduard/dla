{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0f5d0b9d-7980-4d2c-8154-c07a5f8b5525",
      "metadata": {
        "id": "0f5d0b9d-7980-4d2c-8154-c07a5f8b5525"
      },
      "source": [
        "# Introduction\n",
        "In this laboratory we will get our hands dirty working with Large Language Models (e.g. GPT and BERT) to do various useful things. I you haven't already, it is highly recommended to:\n",
        "\n",
        "+ Read the [Attention is All you Need](https://arxiv.org/abs/1706.03762) paper, which is the basis for all transformer-based LLMs.\n",
        "+ Watch (and potentially *code along*) with this [Andrej Karpathy video](https://www.youtube.com/watch?v=kCc8FmEb1nY) which shows you how to build an autoregressive GPT model from the ground up.\n",
        "\n",
        "# Exercise 1: Warming Up\n",
        "In this first exercise you will train a *small* autoregressive GPT model for character generation (the one used by Karpathy in his video) to generate text in the style of Dante Aligheri. Use [this file](https://archive.org/stream/ladivinacommedia00997gut/1ddcd09.txt), which contains the entire text of Dante's Inferno (**note**: you will have to delete some introductory text at the top of the file before training). Train the model for a few epochs, monitor the loss, and generate some text at the end of training. Qualitatively evaluate the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd466d3b-cc41-4de3-9f82-3547569909f0",
      "metadata": {
        "id": "bd466d3b-cc41-4de3-9f82-3547569909f0",
        "outputId": "8c0ddf8d-efc5-412d-a8b4-69c22cfcf318",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10.783546 M parameters\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|                                                  | 0/1000 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 0: train loss 4.0476, val loss 4.0427\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 10%|████                                    | 100/1000 [01:24<05:26,  2.76it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 100: train loss 2.3760, val loss 2.4034\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 20%|████████                                | 200/1000 [02:48<04:45,  2.80it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 200: train loss 2.2835, val loss 2.3137\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 30%|████████████                            | 300/1000 [04:11<04:13,  2.77it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 300: train loss 2.1615, val loss 2.1907\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 40%|████████████████                        | 400/1000 [05:36<03:37,  2.76it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 400: train loss 1.9235, val loss 1.9592\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 50%|████████████████████                    | 500/1000 [07:00<03:00,  2.77it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 500: train loss 1.7546, val loss 1.7993\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 60%|████████████████████████                | 600/1000 [08:24<02:24,  2.76it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 600: train loss 1.6406, val loss 1.7044\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 70%|████████████████████████████            | 700/1000 [09:49<01:48,  2.77it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 700: train loss 1.5294, val loss 1.6300\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 80%|████████████████████████████████        | 800/1000 [11:13<01:12,  2.76it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 800: train loss 1.4382, val loss 1.5861\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 90%|████████████████████████████████████    | 900/1000 [12:37<00:36,  2.76it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 900: train loss 1.3449, val loss 1.5481\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|███████████████████████████████████████▉| 999/1000 [14:01<00:00,  2.76it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 999: train loss 1.2498, val loss 1.5333\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|███████████████████████████████████████| 1000/1000 [14:49<00:00,  1.12it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Di  se tutto ferbele bestilme\n",
            "  seguerta ne l'altemma, utger divesse,\n",
            "  franne siede, acmicola` giu` de' fuggira\n",
            "\n",
            "me le prose tu temo de l'embre e cota;\n",
            "  me m'abbe a da questa ta lietra onde me;\n",
            "  sotto ci solo acqua fella a pie` quica tutti>>.\n",
            "\n",
            "Quinfin sente l'erra viede 'l maestro,\n",
            "  fuol, ch'acqua greffante eratta ciaccia\n",
            "  di foco Il fatte forcese tenduna.\n",
            "\n",
            "E 'nvidi la far sconda lrude gratto\n",
            "  cimo susci strattinia e suscilitsi\n",
            "  da volta vedesta da cio` non che montre\n",
            "\n",
            "facealde sotto Sie\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from tqdm import tqdm\n",
        "# hyperparameters\n",
        "batch_size = 64 # how many independent sequences will we process in parallel?\n",
        "block_size = 256 # what is the maximum context length for predictions?\n",
        "max_iters = 1000\n",
        "eval_interval = 100\n",
        "learning_rate = 3e-4\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 384\n",
        "n_head = 6\n",
        "n_layer = 6\n",
        "dropout = 0.2\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "with open('input2.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # input of size (batch, time-step, channels)\n",
        "        # output of size (batch, time-step, head size)\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,hs)\n",
        "        q = self.query(x) # (B,T,hs)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,hs)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class GPTLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "        # better init, not covered in the original GPT video, but important, will cover in followup video\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = GPTLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in tqdm(range(max_iters)):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))\n",
        "#open('more.txt', 'w').write(decode(m.generate(context, max_new_tokens=10000)[0].tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "657e57ba-5e90-46a0-a4f0-fbbf394d2b9e",
      "metadata": {
        "id": "657e57ba-5e90-46a0-a4f0-fbbf394d2b9e"
      },
      "outputs": [],
      "source": [
        "torch.save(m.state_dict(), \"model.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc3296e7-3ad4-4980-a788-252c6972a1c1",
      "metadata": {
        "id": "dc3296e7-3ad4-4980-a788-252c6972a1c1"
      },
      "outputs": [],
      "source": [
        "for iter in tqdm(range(max_iters)):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1a76f7e-ae7c-4c37-8938-eccce8518774",
      "metadata": {
        "id": "d1a76f7e-ae7c-4c37-8938-eccce8518774",
        "outputId": "dc059fbd-c1a6-480f-d795-31189992bb16"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Inferno: Canto XI\n",
            "\n",
            "\n",
            "Ne' quel che m'abbaianco a si confesse\n",
            "  la` reno`, da lui, si` conforte,\n",
            "  e due 'l seguir qual e` com'uscive.\n",
            "\n",
            "Io savisi poggiati e con piu` dentro;\n",
            "  e di rispontar la coda di spiata\n",
            "  di quell'aere quanto a mul che non modo.\n",
            "\n",
            "Ma per l'occhio scoglio travien sospiri,\n",
            "  a mi cantare un peccatori, a cu' il giuso\n",
            "  con tenea ch'era fatto al cio` ch'i' vinsi.\n",
            "\n",
            "Vedi che disse: \"Qui son piglio convien che e trova\n",
            "  a quel papeccator, e pugno, presso regio\n",
            "  e volte a la` bramos\n"
          ]
        }
      ],
      "source": [
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e95f3ae-ca2e-4f1c-8bb2-5b0daad17641",
      "metadata": {
        "id": "8e95f3ae-ca2e-4f1c-8bb2-5b0daad17641"
      },
      "outputs": [],
      "source": [
        "torch.save(m.state_dict(), \"model2.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68441a09-dfaf-424a-b640-4fc8cea289b5",
      "metadata": {
        "id": "68441a09-dfaf-424a-b640-4fc8cea289b5"
      },
      "source": [
        "# Exercise 2: Working with Real LLMs\n",
        "\n",
        "Our toy GPT can only take us so far. In this exercise we will see how to use the [Hugging Face](https://huggingface.co/) model and dataset ecosystem to access a *huge* variety of pre-trained transformer models.\n",
        "\n",
        "## Exercise 2.1: Installation and text tokenization\n",
        "\n",
        "First things first, we need to install the [Hugging Face transformer library](https://huggingface.co/docs/transformers/index):\n",
        "\n",
        "    conda install -c huggingface -c conda-forge transformers\n",
        "    \n",
        "The key classes that you will work with are `GPT2Tokenizer` to encode text into sub-word tokens, and the `GPT2LMHeadModel`. **Note** the `LMHead` part of the class name -- this is the version of the GPT2 architecture that has the text prediction heads attached to the final hidden layer representations (i.e. what we need to **generate** text).\n",
        "\n",
        "Instantiate the `GPT2Tokenizer` and experiment with encoding text into integer tokens. Compare the length of input with the encoded sequence length.\n",
        "\n",
        "**Tip**: Pass the `return_tensors='pt'` argument to the togenizer to get Pytorch tensors as output (instead of lists)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af199a6d-1f3a-4b2c-a23f-d697b93c5adb",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "919024e580014ebb9d1266fdce374393",
            "5b9d3690c00b4273bb5bfb9fa7bd7696",
            "fac26578819f4089b9e207be3484209d"
          ]
        },
        "id": "af199a6d-1f3a-4b2c-a23f-d697b93c5adb",
        "outputId": "053ad56f-8148-4957-90b9-1c162cad4e5f",
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "919024e580014ebb9d1266fdce374393",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5b9d3690c00b4273bb5bfb9fa7bd7696",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fac26578819f4089b9e207be3484209d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Your code here.\n",
        "from transformers import GPT2Tokenizer\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd89b3d5-d99f-474e-a6b6-b9cab030581d",
      "metadata": {
        "id": "fd89b3d5-d99f-474e-a6b6-b9cab030581d",
        "outputId": "ae5b9c84-3bf3-40b3-b190-137eae757391"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[5779,   11,  994,  356,  389]])"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer(\"Well, here we are\", return_tensors='pt')['input_ids']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a458b725-63c1-49ae-8011-71a9196387b8",
      "metadata": {
        "id": "a458b725-63c1-49ae-8011-71a9196387b8"
      },
      "source": [
        "## Exercise 2.2: Generating Text\n",
        "\n",
        "There are a lot of ways we can, given a *prompt* in input, sample text from a GPT2 model. Instantiate a pre-trained `GPT2LMHeadModel` and use the [`generate()`](https://huggingface.co/docs/transformers/v4.27.2/en/main_classes/text_generation#transformers.GenerationMixin.generate) method to generate text from a prompt.\n",
        "\n",
        "**Note**: The default inference mode for GPT2 is *greedy* which might not results in satisfying generated text. Look at the `do_sample` and `temperature` parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bdad9208-cc9e-4750-baa5-f9367e71362a",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "ca66cb83a4304a68889166dc71256b08"
          ]
        },
        "id": "bdad9208-cc9e-4750-baa5-f9367e71362a",
        "outputId": "eed9bbcc-0bf8-4b34-f5e9-ca0c98f39277",
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ca66cb83a4304a68889166dc71256b08",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/548M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "/home/muduard/anaconda3/envs/gen/lib/python3.10/site-packages/transformers/generation_utils.py:1296: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 20 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor([[50256,   198,   464,   717,   640,   314,  2497,   262,   649,  2196,\n",
              "           286,   262,   983,    11,   314,   373,   523,  6568,    13,   314]])"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import GPT2LMHeadModel\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
        "\n",
        "outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d067484-ff8f-4284-9520-6218ecf8e97a",
      "metadata": {
        "id": "8d067484-ff8f-4284-9520-6218ecf8e97a",
        "outputId": "9e65b8e5-34f8-41d1-f05d-b7dd06d058de"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " now an easy day. And it looks amazing today because we came at 10 PM, we started that 5min time on Thursday 9 to end we had 30mins but by Monday it looks we still ran into 50k-60mins per mile for 24+ times, which in terms people did have fun to work after 10 hours long with no training needed and very interesting challenges after it starts! So there\n"
          ]
        }
      ],
      "source": [
        "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "inputs = tokenizer([\"Today is\"], return_tensors=\"pt\")\n",
        "\n",
        "# Example 1: Print the scores for each token generated with Greedy Search\n",
        "\n",
        "outputs = model.generate(**inputs, max_new_tokens=80, return_dict_in_generate=True, output_scores=True, do_sample=True, temperature=2.3)\n",
        "\n",
        "input_length = 1 if model.config.is_encoder_decoder else inputs.input_ids.shape[1]\n",
        "\n",
        "generated_tokens = outputs.sequences[:, input_length:]\n",
        "for tok in generated_tokens:\n",
        "    print(tokenizer.decode(tok))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b3d00e7-d4db-440f-8702-11118f07b0a4",
      "metadata": {
        "id": "9b3d00e7-d4db-440f-8702-11118f07b0a4"
      },
      "source": [
        "# Exercise 3: Reusing Pre-trained LLMs (choose one)\n",
        "\n",
        "Choose **one** of the following exercises (well, *at least* one). In each of these you are asked to adapt a pre-trained LLM (`GPT2Model` or `DistillBERT` are two good choices) to a new Natural Language Understanding task. A few comments:\n",
        "\n",
        "+ Since GPT2 is a *autoregressive* model, there is no latent space aggregation at the last transformer layer (you get the same number of tokens out that you give in input). To use a pre-trained model for a classification or retrieval task, you should aggregate these tokens somehow (or opportunistically select *one* to use).\n",
        "\n",
        "+ BERT models (including DistillBERT) have a special [CLS] token prepended to each latent representation in output from a self-attention block. You can directly use this as a representation for classification (or retrieval).\n",
        "\n",
        "+ The first *two* exercises below can probably be done *without* any fine-tuning -- that is, just training a shallow MLP to classify or represent with the appropriate loss function.\n",
        "\n",
        "# Exercise 3.1: Training a Text Classifier (easy)\n",
        "\n",
        "Peruse the [text classification datasets on Hugging Face](https://huggingface.co/datasets?task_categories=task_categories:text-classification&sort=downloads). Choose a *moderately* sized dataset and use a LLM to train a classifier to solve the problem.\n",
        "\n",
        "**Note**: A good first baseline for this problem is certainly to use an LLM *exclusively* as a feature extractor and then train a shallow model.\n",
        "\n",
        "# Exercise 3.2: Training a Question Answering Model (harder)\n",
        "\n",
        "Peruse the [multiple choice question answering datasets on Hugging Face](https://huggingface.co/datasets?task_categories=task_categories:multiple-choice&sort=downloads). Chose a *moderately* sized one and train a model to answer contextualized multiple-choice questions. You *might* be able to avoid fine-tuning by training a simple model to *rank* the multiple choices (see margin ranking loss in Pytorch).\n",
        "\n",
        "# Exercise 3.3: Training a Retrieval Model (hardest)\n",
        "\n",
        "The Hugging Face dataset repository contains a large number of [\"text retrieval\" problems](https://huggingface.co/datasets?task_categories=task_categories:text-retrieval&p=1&sort=downloads). These tasks generally require that the model measure *similarity* between text in some metric space -- naively, just a cosine similarity between [CLS] tokens can get you pretty far. Find an interesting retrieval problem and train a model (starting from a pre-trained LLM of course) to solve it.\n",
        "\n",
        "**Tip**: Sometimes identifying the *retrieval* problems in these datasets can be half the challenge. [This dataset](https://huggingface.co/datasets/BeIR/scifact) might be a good starting point."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d89fcfd8",
      "metadata": {
        "id": "d89fcfd8"
      },
      "source": [
        "# Exercise 3.1: Training a Text Classifier\n",
        "I decided to do classic sentiment classification with **IMDB**, a dataset of 25000 highly polar movie reviews where we need to classify if the review is \"Negative\": 0 or \"Positive\": 1 <br>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72a07a28",
      "metadata": {
        "id": "72a07a28"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3fa7b61e",
      "metadata": {
        "id": "3fa7b61e",
        "outputId": "58363fc3-f1c4-4b56-d128-e0fa13728055"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Found cached dataset imdb (/home/muduard/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0)\n",
            "Loading cached shuffled indices for dataset at /home/muduard/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0/cache-9c48ce5d173413c7.arrow\n",
            "Found cached dataset imdb (/home/muduard/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0)\n"
          ]
        }
      ],
      "source": [
        "train_data = load_dataset(\"imdb\", \"plain_text\",split='train').shuffle(seed=42)\n",
        "test_data = load_dataset(\"imdb\", \"plain_text\",split='test')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "291d2f1f",
      "metadata": {
        "id": "291d2f1f"
      },
      "source": [
        "We use BERT uncased for classification.\n",
        "We define a simple preprocessing function that returns a tokenized input for the forward function of the Trainer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ccca8028",
      "metadata": {
        "id": "ccca8028"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "import numpy as np\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "def preprocess_data(examples):\n",
        "  # take a batch of texts\n",
        "  text = examples[\"text\"]\n",
        "  # encode them\n",
        "  encoding = tokenizer(text, padding=\"max_length\", truncation=True, max_length=128)\n",
        "  # add labels\n",
        "  encoding[\"labels\"] = examples[\"label\"]\n",
        "\n",
        "  return encoding\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nnU4-ABXNJQA",
      "metadata": {
        "id": "nnU4-ABXNJQA"
      },
      "source": [
        "Encode the dataset and test that it works correctly, then format it for pytorch use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d9bff8a",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "17385a578b754ae0ba23b08b70d0d2de"
          ]
        },
        "id": "4d9bff8a",
        "outputId": "7ced8157-0ff2-4a4f-caab-c237f15faa97"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading cached processed dataset at /home/muduard/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0/cache-c58134a836ca495e.arrow\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "17385a578b754ae0ba23b08b70d0d2de",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "encoded_train = train_data.map(preprocess_data, batched=True)\n",
        "encoded_test = test_data.map(preprocess_data, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa0605ec",
      "metadata": {
        "id": "aa0605ec",
        "outputId": "ba38e030-e54d-4671-d1c6-b9c548a39c15"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dict_keys(['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'])\n"
          ]
        }
      ],
      "source": [
        "example = encoded_train[0]\n",
        "print(example.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5cff5800",
      "metadata": {
        "id": "5cff5800",
        "outputId": "ae8adc45-25ff-42ca-a0dc-d072f4624dab"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'[CLS] i rented i am curious - yellow from my video store because of all the controversy that surrounded it when it was first released in 1967. i also heard that at first it was seized by u. s. customs if it ever tried to enter this country, therefore being a fan of films considered \" controversial \" i really had to see this for myself. < br / > < br / > the plot is centered around a young swedish drama student named lena who wants to learn everything she can about life. in particular she wants to focus her attentions to making some sort of documentary on what the average swede thought about certain political issues [SEP]'"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.decode(example['input_ids'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8690ff9d",
      "metadata": {
        "id": "8690ff9d"
      },
      "outputs": [],
      "source": [
        "encoded_train.set_format(\"torch\")\n",
        "encoded_test.set_format(\"torch\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "FLTxF-rrNWzZ",
      "metadata": {
        "id": "FLTxF-rrNWzZ"
      },
      "source": [
        "Huggingface provides a way to load pretrained bert-uncased for the specific task of Sequence CLassification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee93a5c7",
      "metadata": {
        "id": "ee93a5c7",
        "outputId": "e739db3b-f668-43f1-981c-5d0ff62d5839"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading configuration file config.json from cache at /home/muduard/.cache/huggingface/hub/models--bert-base-uncased/snapshots/1dbc166cf8765166998eff31ade2eb64c8a40076/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.23.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file model.safetensors from cache at /home/muduard/.cache/huggingface/hub/models--bert-base-uncased/snapshots/1dbc166cf8765166998eff31ade2eb64c8a40076/model.safetensors\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Y7BFK1DmNqiW",
      "metadata": {
        "id": "Y7BFK1DmNqiW"
      },
      "source": [
        "Initialize arguments to pass the trainer, we use the f1 metric as score and finetune bert for 5 epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd2b1456",
      "metadata": {
        "id": "dd2b1456",
        "outputId": "db9ceae2-a261-4f4c-8e34-550975abad07"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
          ]
        }
      ],
      "source": [
        "batch_size = 8\n",
        "metric_name = \"f1\"\n",
        "from tqdm import tqdm\n",
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "args = TrainingArguments(\n",
        "    f\"bert-imdb-classification\",\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    save_strategy = \"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    num_train_epochs=5,\n",
        "    weight_decay=0.01,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=metric_name\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "GyxKicsTN-Vl",
      "metadata": {
        "id": "GyxKicsTN-Vl"
      },
      "source": [
        "Define metrics and function callback for the trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e88f1bd",
      "metadata": {
        "id": "7e88f1bd"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
        "from transformers import EvalPrediction\n",
        "import torch\n",
        "\n",
        "def b_tp(preds, labels):\n",
        "  '''Returns True Positives (TP): count of correct predictions of actual class 1'''\n",
        "  return sum([preds == labels and preds == 1 for preds, labels in zip(preds, labels)])\n",
        "\n",
        "def b_fp(preds, labels):\n",
        "  '''Returns False Positives (FP): count of wrong predictions of actual class 1'''\n",
        "  return sum([preds != labels and preds == 1 for preds, labels in zip(preds, labels)])\n",
        "\n",
        "def b_tn(preds, labels):\n",
        "  '''Returns True Negatives (TN): count of correct predictions of actual class 0'''\n",
        "  return sum([preds == labels and preds == 0 for preds, labels in zip(preds, labels)])\n",
        "\n",
        "def b_fn(preds, labels):\n",
        "  '''Returns False Negatives (FN): count of wrong predictions of actual class 0'''\n",
        "  return sum([preds != labels and preds == 0 for preds, labels in zip(preds, labels)])\n",
        "\n",
        "def b_metrics(preds, labels):\n",
        "  '''\n",
        "  Returns the following metrics:\n",
        "    - accuracy    = (TP + TN) / N\n",
        "    - precision   = TP / (TP + FP)\n",
        "    - recall      = TP / (TP + FN)\n",
        "    - specificity = TN / (TN + FP)\n",
        "  '''\n",
        "  preds = np.argmax(preds, axis = 1).flatten()\n",
        "  labels = labels.flatten()\n",
        "  tp = b_tp(preds, labels)\n",
        "\n",
        "  tn = b_tn(preds, labels)\n",
        "  fp = b_fp(preds, labels)\n",
        "  fn = b_fn(preds, labels)\n",
        "  b_accuracy = (tp + tn) / len(labels)\n",
        "  b_precision = tp / (tp + fp) if (tp + fp) > 0 else 'nan'\n",
        "  b_recall = tp / (tp + fn) if (tp + fn) > 0 else 'nan'\n",
        "  f1 = 2 * tp / (2 * tp + fp + fn)\n",
        "  metrics =  {'f1': f1,\n",
        "               'accuracy': b_accuracy,\n",
        "               'precision': b_precision,\n",
        "               'recall': b_recall}\n",
        "  return metrics\n",
        "\n",
        "# Function to be called by the trainer\n",
        "def compute_metrics(p: EvalPrediction):\n",
        "    preds = p.predictions[0] if isinstance(p.predictions,\n",
        "            tuple) else p.predictions\n",
        "    result = b_metrics(\n",
        "        preds=preds,\n",
        "        labels=p.label_ids)\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dSFyoxIaOX2z",
      "metadata": {
        "id": "dSFyoxIaOX2z"
      },
      "source": [
        "Use the all in one Trainer of huggingface for finetuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7549e43f",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "329ccfe6a28f49998884c58e656a9c48",
            "05a992c35d604193be520353c37d1d2a",
            "1fa3ed1ebe614133a1fd7837ffa4dfb3",
            "11e48043c07e444eaab39ede4f4176c6",
            "344f41e2496348a9a1b26adc12672302",
            "e59310a4f9504039bb4e39f905cdb694"
          ]
        },
        "id": "7549e43f",
        "outputId": "88f6d5cd-deb5-4db6-dd11-ec005a5b0c8b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "/home/muduard/anaconda3/envs/gen/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 25000\n",
            "  Num Epochs = 5\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 15625\n",
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "329ccfe6a28f49998884c58e656a9c48",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/15625 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.4275, 'learning_rate': 1.936e-05, 'epoch': 0.16}\n",
            "{'loss': 0.3626, 'learning_rate': 1.8720000000000004e-05, 'epoch': 0.32}\n",
            "{'loss': 0.3528, 'learning_rate': 1.8080000000000003e-05, 'epoch': 0.48}\n",
            "{'loss': 0.3536, 'learning_rate': 1.7440000000000002e-05, 'epoch': 0.64}\n",
            "{'loss': 0.358, 'learning_rate': 1.6800000000000002e-05, 'epoch': 0.8}\n",
            "{'loss': 0.348, 'learning_rate': 1.616e-05, 'epoch': 0.96}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 25000\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "05a992c35d604193be520353c37d1d2a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/3125 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to bert-imdb-classification/checkpoint-3125\n",
            "Configuration saved in bert-imdb-classification/checkpoint-3125/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.28213560581207275, 'eval_f1': 0.8837697179973373, 'eval_accuracy': 0.88476, 'eval_precision': 0.8914299666313991, 'eval_recall': 0.87624, 'eval_runtime': 138.1343, 'eval_samples_per_second': 180.983, 'eval_steps_per_second': 22.623, 'epoch': 1.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in bert-imdb-classification/checkpoint-3125/pytorch_model.bin\n",
            "tokenizer config file saved in bert-imdb-classification/checkpoint-3125/tokenizer_config.json\n",
            "Special tokens file saved in bert-imdb-classification/checkpoint-3125/special_tokens_map.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.2584, 'learning_rate': 1.552e-05, 'epoch': 1.12}\n",
            "{'loss': 0.2522, 'learning_rate': 1.4880000000000002e-05, 'epoch': 1.28}\n",
            "{'loss': 0.2445, 'learning_rate': 1.4240000000000001e-05, 'epoch': 1.44}\n",
            "{'loss': 0.2464, 'learning_rate': 1.3600000000000002e-05, 'epoch': 1.6}\n",
            "{'loss': 0.2327, 'learning_rate': 1.2960000000000001e-05, 'epoch': 1.76}\n",
            "{'loss': 0.2555, 'learning_rate': 1.232e-05, 'epoch': 1.92}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 25000\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1fa3ed1ebe614133a1fd7837ffa4dfb3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/3125 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to bert-imdb-classification/checkpoint-6250\n",
            "Configuration saved in bert-imdb-classification/checkpoint-6250/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.4703359305858612, 'eval_f1': 0.889341584348695, 'eval_accuracy': 0.88484, 'eval_precision': 0.8558851816231412, 'eval_recall': 0.92552, 'eval_runtime': 145.0214, 'eval_samples_per_second': 172.388, 'eval_steps_per_second': 21.549, 'epoch': 2.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in bert-imdb-classification/checkpoint-6250/pytorch_model.bin\n",
            "tokenizer config file saved in bert-imdb-classification/checkpoint-6250/tokenizer_config.json\n",
            "Special tokens file saved in bert-imdb-classification/checkpoint-6250/special_tokens_map.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.1775, 'learning_rate': 1.168e-05, 'epoch': 2.08}\n",
            "{'loss': 0.1128, 'learning_rate': 1.1040000000000001e-05, 'epoch': 2.24}\n",
            "{'loss': 0.1427, 'learning_rate': 1.04e-05, 'epoch': 2.4}\n",
            "{'loss': 0.1298, 'learning_rate': 9.760000000000001e-06, 'epoch': 2.56}\n",
            "{'loss': 0.149, 'learning_rate': 9.12e-06, 'epoch': 2.72}\n",
            "{'loss': 0.1278, 'learning_rate': 8.48e-06, 'epoch': 2.88}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 25000\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "11e48043c07e444eaab39ede4f4176c6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/3125 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to bert-imdb-classification/checkpoint-9375\n",
            "Configuration saved in bert-imdb-classification/checkpoint-9375/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.5590696334838867, 'eval_f1': 0.8859392758564703, 'eval_accuracy': 0.8876, 'eval_precision': 0.8992254449571523, 'eval_recall': 0.87304, 'eval_runtime': 138.8808, 'eval_samples_per_second': 180.01, 'eval_steps_per_second': 22.501, 'epoch': 3.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in bert-imdb-classification/checkpoint-9375/pytorch_model.bin\n",
            "tokenizer config file saved in bert-imdb-classification/checkpoint-9375/tokenizer_config.json\n",
            "Special tokens file saved in bert-imdb-classification/checkpoint-9375/special_tokens_map.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.1079, 'learning_rate': 7.840000000000001e-06, 'epoch': 3.04}\n",
            "{'loss': 0.0606, 'learning_rate': 7.2000000000000005e-06, 'epoch': 3.2}\n",
            "{'loss': 0.081, 'learning_rate': 6.560000000000001e-06, 'epoch': 3.36}\n",
            "{'loss': 0.0662, 'learning_rate': 5.92e-06, 'epoch': 3.52}\n",
            "{'loss': 0.0594, 'learning_rate': 5.28e-06, 'epoch': 3.68}\n",
            "{'loss': 0.0678, 'learning_rate': 4.6400000000000005e-06, 'epoch': 3.84}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 25000\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0513, 'learning_rate': 4.000000000000001e-06, 'epoch': 4.0}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "344f41e2496348a9a1b26adc12672302",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/3125 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to bert-imdb-classification/checkpoint-12500\n",
            "Configuration saved in bert-imdb-classification/checkpoint-12500/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.6741758584976196, 'eval_f1': 0.8881728129848431, 'eval_accuracy': 0.88756, 'eval_precision': 0.8833583920234233, 'eval_recall': 0.89304, 'eval_runtime': 140.1278, 'eval_samples_per_second': 178.409, 'eval_steps_per_second': 22.301, 'epoch': 4.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in bert-imdb-classification/checkpoint-12500/pytorch_model.bin\n",
            "tokenizer config file saved in bert-imdb-classification/checkpoint-12500/tokenizer_config.json\n",
            "Special tokens file saved in bert-imdb-classification/checkpoint-12500/special_tokens_map.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0273, 'learning_rate': 3.3600000000000004e-06, 'epoch': 4.16}\n",
            "{'loss': 0.0401, 'learning_rate': 2.7200000000000002e-06, 'epoch': 4.32}\n",
            "{'loss': 0.022, 'learning_rate': 2.08e-06, 'epoch': 4.48}\n",
            "{'loss': 0.0256, 'learning_rate': 1.44e-06, 'epoch': 4.64}\n",
            "{'loss': 0.0258, 'learning_rate': 8.000000000000001e-07, 'epoch': 4.8}\n",
            "{'loss': 0.0299, 'learning_rate': 1.6e-07, 'epoch': 4.96}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 25000\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e59310a4f9504039bb4e39f905cdb694",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/3125 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to bert-imdb-classification/checkpoint-15625\n",
            "Configuration saved in bert-imdb-classification/checkpoint-15625/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.7640025019645691, 'eval_f1': 0.8867014865407794, 'eval_accuracy': 0.8872, 'eval_precision': 0.890637610976594, 'eval_recall': 0.8828, 'eval_runtime': 140.3425, 'eval_samples_per_second': 178.136, 'eval_steps_per_second': 22.267, 'epoch': 5.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in bert-imdb-classification/checkpoint-15625/pytorch_model.bin\n",
            "tokenizer config file saved in bert-imdb-classification/checkpoint-15625/tokenizer_config.json\n",
            "Special tokens file saved in bert-imdb-classification/checkpoint-15625/special_tokens_map.json\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from bert-imdb-classification/checkpoint-6250 (score: 0.889341584348695).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'train_runtime': 3251.3895, 'train_samples_per_second': 38.445, 'train_steps_per_second': 4.806, 'train_loss': 0.1664598349761963, 'epoch': 5.0}\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=15625, training_loss=0.1664598349761963, metrics={'train_runtime': 3251.3895, 'train_samples_per_second': 38.445, 'train_steps_per_second': 4.806, 'train_loss': 0.1664598349761963, 'epoch': 5.0})"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=encoded_train,\n",
        "    eval_dataset=encoded_test,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "#trainer.train(\"imdb\") # for pretrained model\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "TiVVrXqAOguI",
      "metadata": {
        "id": "TiVVrXqAOguI"
      },
      "source": [
        "We have overfitting after the second epoch (checkpoint-6250) and with just 2 epochs of finetuning we achieve an f1-score of 0.89 and an accuracy of 0.885. This shows how powerful is Bert for simple classification tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3JiGtD0EPe1L",
      "metadata": {
        "id": "3JiGtD0EPe1L"
      },
      "source": [
        "If we wanted to do inference on the fine-tuned model we could do it like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62d19360",
      "metadata": {
        "id": "62d19360",
        "outputId": "2f8881cd-6712-4e87-f1d0-d3747334f934"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([1], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "text = \"This movie was very beatiful and sad\"\n",
        "\n",
        "encoding = tokenizer(text, return_tensors=\"pt\")\n",
        "encoding = {k: v.to(trainer.model.device) for k,v in encoding.items()}\n",
        "\n",
        "outputs = trainer.model(**encoding)\n",
        "logits = outputs.logits\n",
        "preds = torch.argmax(logits, axis = 1).flatten()\n",
        "print(preds.item())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da353191",
      "metadata": {
        "id": "da353191"
      },
      "source": [
        "# Exercise 3.2: Training a Question Answering model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BT5iL3OHk62a",
      "metadata": {
        "id": "BT5iL3OHk62a"
      },
      "source": [
        "I train a question answering model on the [medmcqa](https://github.com/medmcqa/medmcqa) dataset, a really difficult multiple choice answer dataset. The current best accuracy is 0.72 by Med-Palm 2 and the expected accuracy of a bert-base model is 0.33 (Just slightly above a random choice)\n",
        "I still try fine-tuning distill-bert-cased for this task to check if I can reproduce the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5691f4b8",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "b83b79f4f3c440628ce777bbfeb7ddf0",
            "cf97ab96aeb04116bd6bdf8a56478b8e",
            "f1d627ca921e45bd90ba5f5f9c5f8860",
            "7ee8df2c3fb94927b7906322b07c92ba",
            "6a577e0f261e4e34a65da48f43a5f80c"
          ]
        },
        "id": "5691f4b8",
        "outputId": "fea8ad19-4ceb-4418-f803-52ccaca8accb"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "import numpy as np\n",
        "dataset = load_dataset(\"medmcqa\")\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-cased\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yDjbxmj0lm0q",
      "metadata": {
        "id": "yDjbxmj0lm0q"
      },
      "source": [
        "We define a preprocessing function similar to the first exercise but this time we encode a pair of (question,answer) for every choice, then we set the correct label as classification objective."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86d89614",
      "metadata": {
        "id": "86d89614"
      },
      "outputs": [],
      "source": [
        "endings = [\"opa\",\"opb\",\"opc\",\"opd\"]\n",
        "def preprocess_function(examples):\n",
        "\n",
        "    first_sentences = [[context] * 4 for context in examples[\"question\"]]\n",
        "    second_sentences = [\n",
        "        [examples[end][i] for end in endings] for i, _ in enumerate(examples['id'])\n",
        "    ]\n",
        "    first_sentences = sum(first_sentences, [])\n",
        "    second_sentences = sum(second_sentences, [])\n",
        "    tokenized_examples = tokenizer(first_sentences, second_sentences, truncation=True)\n",
        "    encoding = {k: [v[i: i + 4] for i in range(0, len(v), 4)] for k, v in tokenized_examples.items()}\n",
        "    encoding['label'] = examples['cop']\n",
        "    return encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ax8wVscFmD2I",
      "metadata": {
        "id": "ax8wVscFmD2I"
      },
      "source": [
        "Encode dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "666c213a",
      "metadata": {
        "id": "666c213a"
      },
      "outputs": [],
      "source": [
        "tokenized_dataset = dataset.map(preprocess_function, batched=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Qy7nsM-WmVPg",
      "metadata": {
        "id": "Qy7nsM-WmVPg"
      },
      "source": [
        "We define a data collator to pad the multiple choices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94759e96",
      "metadata": {
        "id": "94759e96"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\n",
        "from typing import Optional, Union\n",
        "import torch\n",
        "\n",
        "#Adapted from https://huggingface.co/docs/transformers/tasks/multiple_choice\n",
        "@dataclass\n",
        "class DataCollatorForMultipleChoice:\n",
        "\n",
        "    tokenizer: PreTrainedTokenizerBase\n",
        "\n",
        "    padding: Union[bool, str, PaddingStrategy] = True\n",
        "\n",
        "    max_length: Optional[int] = None\n",
        "\n",
        "    pad_to_multiple_of: Optional[int] = None\n",
        "\n",
        "    def __call__(self, features):\n",
        "        label_name = \"label\"\n",
        "        labels = [feature.pop(label_name) for feature in features]\n",
        "        batch_size = len(features)\n",
        "        num_choices = len(features[0][\"input_ids\"])\n",
        "        flattened_features = [\n",
        "            [{k: v[i] for k, v in feature.items()} for i in range(num_choices)] for feature in features\n",
        "        ]\n",
        "        flattened_features = sum(flattened_features, [])\n",
        "\n",
        "        batch = self.tokenizer.pad(\n",
        "            flattened_features,\n",
        "            padding=self.padding,\n",
        "            max_length=self.max_length,\n",
        "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        batch = {k: v.view(batch_size, num_choices, -1) for k, v in batch.items()}\n",
        "        batch[\"labels\"] = torch.tensor(labels, dtype=torch.int64)\n",
        "        return batch"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8EJyHtgYmrVH",
      "metadata": {
        "id": "8EJyHtgYmrVH"
      },
      "source": [
        "Define metrics for evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yJKCopfemmlJ",
      "metadata": {
        "id": "yJKCopfemmlJ"
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "\n",
        "accuracy = evaluate.load(\"accuracy\")\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    return accuracy.compute(predictions=predictions, references=labels)\n",
        "\n",
        "\n",
        "from transformers import AutoModelForMultipleChoice, TrainingArguments, Trainer\n",
        "\n",
        "model = AutoModelForMultipleChoice.from_pretrained(\"distilbert-base-cased\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "IbZMxYgdo_ot",
      "metadata": {
        "id": "IbZMxYgdo_ot"
      },
      "source": [
        "Setup training arguments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PXoKw9tuo_Fs",
      "metadata": {
        "id": "PXoKw9tuo_Fs"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"medqa_model\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    num_train_epochs=6,\n",
        "    weight_decay=0.01,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset[\"train\"],\n",
        "    eval_dataset=tokenized_dataset[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=DataCollatorForMultipleChoice(tokenizer=tokenizer),\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2MikSTDRpDL8",
      "metadata": {
        "id": "2MikSTDRpDL8"
      },
      "outputs": [],
      "source": [
        "#trainer.train('medqa_model') #for pretrained model\n",
        "trainer.train()\n",
        "trainer.save_state()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_JGo-BXmpHL7",
      "metadata": {
        "id": "_JGo-BXmpHL7"
      },
      "source": [
        "After fine-tuning for 5 epochs we obtain an accuracy on the validation set of 0.321, this is in line with the bert-uncased training cited in the paper."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "699bb1ec",
      "metadata": {
        "id": "699bb1ec"
      },
      "source": [
        "# Exercise 3.3: Training a Retrieval Model\n",
        "I approached text retrieval from an interesting standpoint: Using a Sequence-to-Sequence model.<br>\n",
        "The model used is the recently proposed Flan-T5 [[Scaling Instruction-Finetuned Language Models](https://arxiv.org/abs/2210.11416)], an improvement over the commonly used T5 model [[Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/abs/1910.10683)] from Google.<br>\n",
        "Searching for ways to implement text retrieval, I found the one described in the paper [[Document Ranking with a Pretrained Sequence-to-Sequence Model](https://aclanthology.org/2020.findings-emnlp.63/)] particurarly ingenious. <br>\n",
        "The entire premise is training the model to complete a sentence of the type:<br>\n",
        "`Query [Q] Document [D] Relevant:` <br>\n",
        "The model is trained to predict \"true\" or \"false\" and the score associated is the softmax of the logits \"true\" and \"false\":\n",
        "$$ Pr(relevant = 1 \\vert q,d)$$\n",
        "The training dataset is a subset of MS Marco from the BEIR benchmark on information retrieval tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0a4e635",
      "metadata": {
        "id": "e0a4e635"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration, PreTrainedModel, PreTrainedTokenizer\n",
        "from datasets import load_dataset\n",
        "from copy import deepcopy\n",
        "import heapq\n",
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "from collections.abc import Iterable\n",
        "from tqdm.auto import tqdm\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import argparse\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    Seq2SeqTrainer,\n",
        "    Seq2SeqTrainingArguments,\n",
        ")\n",
        "from typing import List, Optional, Union, Mapping, Any\n",
        "from torch.utils.data import Dataset\n",
        "from dataclasses import dataclass"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5XpIKfLIP1iL",
      "metadata": {
        "id": "5XpIKfLIP1iL"
      },
      "source": [
        "I use some helper classes from PyGaggle to simplify the reranking procedure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2b770ab",
      "metadata": {
        "id": "e2b770ab"
      },
      "outputs": [],
      "source": [
        "# Define helper classes adapted from https://github.com/castorini/pygaggle/\n",
        "class MonoT5Dataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.data[idx]\n",
        "        text = f'Query: {sample[0]} Document: {sample[1]} Relevant:'\n",
        "        return {\n",
        "            'labels': sample[2],\n",
        "            'input_ids': text,\n",
        "        }\n",
        "\n",
        "\n",
        "class Text:\n",
        "    def __init__(self,\n",
        "                 text: str,\n",
        "                 metadata: Mapping[str, Any] = None,\n",
        "                 score: Optional[float] = 0,\n",
        "                 title: Optional[str] = None):\n",
        "        self.text = text\n",
        "        if metadata is None:\n",
        "            metadata = dict()\n",
        "        self.metadata = metadata\n",
        "        self.score = score\n",
        "        self.title = title\n",
        "\n",
        "class Query:\n",
        "    def __init__(self, text: str, id: Optional[str] = None):\n",
        "        self.text = text\n",
        "        self.id = id\n",
        "\n",
        "TokenizerReturnType = Mapping[str, Union[torch.Tensor, List[int],\n",
        "                                         List[List[int]],\n",
        "                                         List[List[str]]]]\n",
        "\n",
        "@dataclass\n",
        "class QueryDocumentBatch:\n",
        "    query: Query\n",
        "    documents: List[Text]\n",
        "    output: Optional[TokenizerReturnType] = None\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.documents)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "x6iuOfagPzgF",
      "metadata": {
        "id": "x6iuOfagPzgF"
      },
      "source": [
        "To train a reranking model we need to generate some negative samples because the model can't learn from only positive examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3d4aab8",
      "metadata": {
        "id": "c3d4aab8"
      },
      "outputs": [],
      "source": [
        "# Training\n",
        "def train(args):\n",
        "    device = torch.device('cuda')\n",
        "    torch.manual_seed(123)\n",
        "    # Use flan-t5-base instead of t5\n",
        "    tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-base\")\n",
        "    model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-base\", device_map=\"auto\")\n",
        "\n",
        "    # Load datasets\n",
        "    # Contains queries\n",
        "    queries = load_dataset(\"BeIR/msmarco\", 'queries')['queries']\n",
        "    # Contains documents\n",
        "    corpus = load_dataset(\"BeIR/msmarco\", 'corpus')['corpus']\n",
        "    # Contains pre-computed relevance between queries and documents\n",
        "    qrels = load_dataset(\"BeIR/msmarco-qrels\", split=\"train\")\n",
        "\n",
        "    # Count number of occurance of feature in dataset\n",
        "    def count_feature(dataset, feature_name):\n",
        "        count_dict = {}\n",
        "        for data in dataset:\n",
        "            if data[feature_name] in count_dict:\n",
        "                count_dict[data[feature_name]] += 1\n",
        "            else:\n",
        "                count_dict[data[feature_name]] = 0\n",
        "        return count_dict\n",
        "\n",
        "    # Get topk elements of dict\n",
        "    def get_topk(count_dict, k):\n",
        "        topk = heapq.nlargest(k, count_dict, key=count_dict.get)\n",
        "        return topk\n",
        "\n",
        "\n",
        "    # Generates samples\n",
        "    def generate_samples(out_path):\n",
        "\n",
        "        train_samples = []\n",
        "        # If pickle already exists, don't recompute\n",
        "        if os.path.exists(out_path):\n",
        "            with open(out_path, \"rb\") as fp:\n",
        "                train_samples = pickle.load(fp)\n",
        "        else:\n",
        "            # Count number of times documents appears in the relation db\n",
        "            count_docs_rel = count_feature(qrels, 'corpus-id')\n",
        "            # Get top 10000 cited documents\n",
        "            doc_ids = get_topk(count_docs_rel, 10000)\n",
        "\n",
        "            for rel in tqdm(qrels, desc=\"Generating samples:\"):\n",
        "                query = rel['query-id']\n",
        "                positive = rel['corpus-id']\n",
        "                train_samples.append((query, positive, 'true'))\n",
        "                # Generate random negative sample\n",
        "                negatives = np.random.choice(doc_ids)\n",
        "                train_samples.append((query, negatives, 'false'))\n",
        "            with open(out_path, \"wb\") as fp:\n",
        "                pickle.dump(train_samples, fp)\n",
        "        return train_samples\n",
        "\n",
        "    def get_texts_from_ids(dataset, column_name):\n",
        "        q_pd = dataset.to_pandas()\n",
        "        q_pd['_id'] = q_pd['_id'].astype(int)\n",
        "        return q_pd.loc[q_pd['_id'].isin(qrels[column_name])]['text'].to_list()\n",
        "\n",
        "    id_samples = generate_samples('./samples.pkl')\n",
        "    q_pd = queries.to_pandas()\n",
        "    q_pd['_id'] = q_pd['_id'].astype(int)\n",
        "    c_pd = corpus.to_pandas()\n",
        "    c_pd['_id'] = c_pd['_id'].astype(int)\n",
        "    def get_q(id):\n",
        "        return q_pd.loc[q_pd['_id'] == id]['text'].item()\n",
        "    def get_c(id):\n",
        "        return c_pd.loc[c_pd['_id'] == id]['text'].item()\n",
        "\n",
        "    train_samples = []\n",
        "    # Get already generated train samples\n",
        "    if os.path.exists('train_samples.pkl'):\n",
        "        with open('train_samples.pkl', \"rb\") as fp:\n",
        "            train_samples = pickle.load(fp)\n",
        "\n",
        "    # Custom function for batch collate\n",
        "    def smart_batching_collate_text_only(batch):\n",
        "        texts = [example['input_ids'] for example in batch]\n",
        "        tokenized = tokenizer(texts, padding=True, truncation='longest_first', return_tensors='pt', max_length=512)\n",
        "        tokenized['labels'] = tokenizer([example['labels'] for example in batch], return_tensors='pt')['input_ids']\n",
        "\n",
        "        for name in tokenized:\n",
        "            tokenized[name] = tokenized[name].to(device)\n",
        "\n",
        "        return tokenized\n",
        "\n",
        "    dataset_train = MonoT5Dataset(train_samples)\n",
        "\n",
        "    if args.save_every_n_steps:\n",
        "        steps = args.save_every_n_steps\n",
        "        strategy = 'steps'\n",
        "    else:\n",
        "        steps = 1\n",
        "        strategy = 'epoch'\n",
        "\n",
        "    train_args = Seq2SeqTrainingArguments(\n",
        "        output_dir=args.output_model_path,\n",
        "        do_train=True,\n",
        "        save_strategy=strategy,\n",
        "        save_steps = steps,\n",
        "        logging_steps=args.logging_steps,\n",
        "        per_device_train_batch_size=args.per_device_train_batch_size,\n",
        "        gradient_accumulation_steps=args.gradient_accumulation_steps,\n",
        "        learning_rate=args.learning_rate,\n",
        "        weight_decay=5e-5,\n",
        "        num_train_epochs=1,\n",
        "        warmup_steps=1000,\n",
        "        seed=1,\n",
        "        disable_tqdm=False,\n",
        "        load_best_model_at_end=False,\n",
        "        predict_with_generate=True,\n",
        "        dataloader_pin_memory=False,\n",
        "    )\n",
        "\n",
        "    trainer = Seq2SeqTrainer(\n",
        "        model=model,\n",
        "        args=train_args,\n",
        "        train_dataset=dataset_train,\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=smart_batching_collate_text_only,\n",
        "\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "\n",
        "    trainer.save_model(args.output_model_path)\n",
        "    trainer.save_state()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e17d7eb7",
      "metadata": {
        "id": "e17d7eb7"
      },
      "outputs": [],
      "source": [
        "# Helper classes for inference\n",
        "class TokenizerEncodeMixin:\n",
        "    tokenizer: PreTrainedTokenizer = None\n",
        "    tokenizer_kwargs = None\n",
        "\n",
        "    def encode(self, strings: List[str]):\n",
        "        assert self.tokenizer and self.tokenizer_kwargs is not None, \\\n",
        "                'mixin used improperly'\n",
        "        ret = self.tokenizer.batch_encode_plus(strings,\n",
        "                                               **self.tokenizer_kwargs)\n",
        "        ret['tokens'] = list(map(self.tokenizer.tokenize, strings))\n",
        "        return ret\n",
        "\n",
        "class QueryDocumentBatchTokenizer(TokenizerEncodeMixin):\n",
        "    def __init__(self,\n",
        "                 tokenizer: PreTrainedTokenizer,\n",
        "                 batch_size: int,\n",
        "                 pattern: str = '{query} {document}',\n",
        "                 **tokenizer_kwargs):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.batch_size = batch_size\n",
        "        self.tokenizer_kwargs = tokenizer_kwargs\n",
        "        self.pattern = pattern\n",
        "\n",
        "    def traverse_query_document(\n",
        "            self,\n",
        "            batch_input: QueryDocumentBatch) -> Iterable[QueryDocumentBatch]:\n",
        "        query = batch_input.query\n",
        "        for batch_idx in range(0, len(batch_input), self.batch_size):\n",
        "            docs = batch_input.documents[batch_idx:batch_idx + self.batch_size]\n",
        "            outputs = self.encode([self.pattern.format(\n",
        "                                        query=query.text,\n",
        "                                        document=doc.text) for doc in docs])\n",
        "            yield QueryDocumentBatch(query, docs, outputs)\n",
        "\n",
        "class T5BatchTokenizer(QueryDocumentBatchTokenizer):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        kwargs['pattern'] = 'Query: {query} Document: {document} Relevant:'\n",
        "        if 'return_attention_mask' not in kwargs:\n",
        "            kwargs['return_attention_mask'] = True\n",
        "        if 'padding' not in kwargs:\n",
        "            kwargs['padding'] = 'longest'\n",
        "        if 'truncation' not in kwargs:\n",
        "            kwargs['truncation'] = True\n",
        "        if 'return_tensors' not in kwargs:\n",
        "            kwargs['return_tensors'] = 'pt'\n",
        "        if 'max_length' not in kwargs:\n",
        "            kwargs['max_length'] = 512\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "\n",
        "class MonoT5():\n",
        "    def __init__(self,\n",
        "                 pretrained_model_name_or_path: str  = 'castorini/monot5-base-msmarco-10k',\n",
        "                 model: T5ForConditionalGeneration = None,\n",
        "                 tokenizer: QueryDocumentBatchTokenizer = None,\n",
        "                 token_false = None,\n",
        "                 token_true  = None):\n",
        "        self.model = model or self.get_model(pretrained_model_name_or_path)\n",
        "        self.tokenizer = tokenizer or self.get_tokenizer(pretrained_model_name_or_path)\n",
        "        self.token_false_id, self.token_true_id = self.get_prediction_tokens(\n",
        "                pretrained_model_name_or_path, self.tokenizer, token_false, token_true)\n",
        "        self.pretrained_model_name_or_path = pretrained_model_name_or_path\n",
        "        self.device = next(self.model.parameters(), None).device\n",
        "\n",
        "    @staticmethod\n",
        "    def get_model(pretrained_model_name_or_path: str,\n",
        "                  *args, device: str = None, **kwargs) -> T5ForConditionalGeneration:\n",
        "        device = device or ('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        device = torch.device(device)\n",
        "        return AutoModelForSeq2SeqLM.from_pretrained(pretrained_model_name_or_path,\n",
        "                                                          *args, **kwargs).to(device).eval()\n",
        "\n",
        "    @staticmethod\n",
        "    def get_tokenizer(pretrained_model_name_or_path: str,\n",
        "                      *args, batch_size: int = 8, **kwargs) -> T5BatchTokenizer:\n",
        "        return T5BatchTokenizer(\n",
        "            AutoTokenizer.from_pretrained(pretrained_model_name_or_path, use_fast=False, *args, **kwargs),\n",
        "            batch_size=batch_size\n",
        "        )\n",
        "    @staticmethod\n",
        "    def get_prediction_tokens(pretrained_model_name_or_path: str,\n",
        "            tokenizer, token_false, token_true):\n",
        "        if not (token_false and token_true):\n",
        "            if pretrained_model_name_or_path in prediction_tokens:\n",
        "                token_false, token_true = prediction_tokens[pretrained_model_name_or_path]\n",
        "                token_false_id = tokenizer.tokenizer.get_vocab()[token_false]\n",
        "                token_true_id  = tokenizer.tokenizer.get_vocab()[token_true]\n",
        "                return token_false_id, token_true_id\n",
        "            else:\n",
        "                raise Exception(f\"We don't know the indexes for the non-relevant/relevant tokens for\\\n",
        "                        the checkpoint {pretrained_model_name_or_path} and you did not provide any.\")\n",
        "        else:\n",
        "            token_false_id = tokenizer.tokenizer.get_vocab()[token_false]\n",
        "            token_true_id  = tokenizer.tokenizer.get_vocab()[token_true]\n",
        "            return token_false_id, token_true_id\n",
        "\n",
        "\n",
        "    def rescore(self, query: Query, texts: List[Text]) -> List[Text]:\n",
        "        texts = deepcopy(texts)\n",
        "        batch_input = QueryDocumentBatch(query=query, documents=texts)\n",
        "        for batch in self.tokenizer.traverse_query_document(batch_input):\n",
        "\n",
        "            input_ids = batch.output['input_ids'].to(self.device)\n",
        "            attn_mask = batch.output['attention_mask'].to(self.device)\n",
        "            batch_scores = greedy_decode(self.model,\n",
        "                                            input_ids,\n",
        "                                            length=1,\n",
        "                                            attention_mask=attn_mask)\n",
        "\n",
        "            batch_scores = batch_scores[:, [self.token_false_id, self.token_true_id]]\n",
        "            # Added temperature of 1.5 to increase entropy and separate better the results\n",
        "            # Flan-t5 model has more uniform logits than t5\n",
        "            batch_scores = torch.nn.functional.log_softmax(batch_scores/1.5, dim=1)\n",
        "            batch_log_probs = batch_scores[:, 1].tolist()\n",
        "            for doc, score in zip(batch.documents, batch_log_probs):\n",
        "                doc.score = score\n",
        "\n",
        "        return texts\n",
        "\n",
        "    def rerank(self, query: Query, texts: List[Text]) -> List[Text]:\n",
        "        return sorted(self.rescore(query, texts), key=lambda x: x.score, reverse=True)\n",
        "\n",
        "@torch.no_grad()\n",
        "def greedy_decode(model: PreTrainedModel,\n",
        "                  input_ids: torch.Tensor,\n",
        "                  length: int,\n",
        "                  attention_mask: torch.Tensor = None):\n",
        "    decode_ids = torch.full((input_ids.size(0), 1),\n",
        "                            model.config.decoder_start_token_id,\n",
        "                            dtype=torch.long).to(input_ids.device)\n",
        "    encoder_outputs = model.get_encoder()(input_ids, attention_mask=attention_mask)\n",
        "    next_token_logits = None\n",
        "    for _ in range(length):\n",
        "        model_inputs = model.prepare_inputs_for_generation(\n",
        "            decode_ids,\n",
        "            encoder_outputs=encoder_outputs,\n",
        "            past=None,\n",
        "            attention_mask=attention_mask,\n",
        "            use_cache=True)\n",
        "        outputs = model(**model_inputs)  # (batch_size, cur_len, vocab_size)\n",
        "        next_token_logits = outputs[0][:, -1, :]  # (batch_size, vocab_size)\n",
        "    return next_token_logits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd643e3c",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "8107c653ed14492aa5353ac9a879890e"
          ]
        },
        "id": "dd643e3c",
        "outputId": "79c4d91f-f36c-4841-826a-fe21fa465ca4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Found cached dataset msmarco (/home/muduard/.cache/huggingface/datasets/BeIR___msmarco/corpus/0.0.0/093f1fe2ffa7a9c72fa48239c8f279b51d6b171abd77737c7fd1406125307599)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8107c653ed14492aa5353ac9a879890e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading configuration file ./checkpoint/config.json\n",
            "Model config T5Config {\n",
            "  \"_name_or_path\": \"google/flan-t5-base\",\n",
            "  \"architectures\": [\n",
            "    \"T5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"n_positions\": 512,\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 200,\n",
            "      \"min_length\": 30,\n",
            "      \"no_repeat_ngram_size\": 3,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"summarize: \"\n",
            "    },\n",
            "    \"translation_en_to_de\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to German: \"\n",
            "    },\n",
            "    \"translation_en_to_fr\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to French: \"\n",
            "    },\n",
            "    \"translation_en_to_ro\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to Romanian: \"\n",
            "    }\n",
            "  },\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.23.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "loading weights file ./checkpoint/pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
            "\n",
            "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at ./checkpoint.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
            "loading file spiece.model from cache at /home/muduard/.cache/huggingface/hub/models--google--flan-t5-base/snapshots/c782cba52f8ea6a704240578055cf1c3fc2f2ca9/spiece.model\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at /home/muduard/.cache/huggingface/hub/models--google--flan-t5-base/snapshots/c782cba52f8ea6a704240578055cf1c3fc2f2ca9/special_tokens_map.json\n",
            "loading file tokenizer_config.json from cache at /home/muduard/.cache/huggingface/hub/models--google--flan-t5-base/snapshots/c782cba52f8ea6a704240578055cf1c3fc2f2ca9/tokenizer_config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 1 7744105         -0.00019 For Earth-centered it was  Geocentric Theory proposed by greeks under the guidance of Ptolemy and Sun-centered was Heliocentric theory proposed by Nicolas Copernicus in 16th century A.D. In short, Your Answers are: 1st blank - Geo-Centric Theory. 2nd blank - Heliocentric Theory.\n",
            " 2 247             -0.04919 Keynesian economics gets its name, theories, and principles from British economist John Maynard Keynes (1883â1946), who is regarded as the founder of modern macroeconomics. His most famous work, The General Theory of Employment, Interest and Money, was published in 1936.\n",
            " 3 248             -0.31595 DEFINITION of 'Keynesian Economics'. An economic theory of total spending in the economy and its effects on output and inflation. Keynesian economics was developed by the British economist John Maynard Keynes during the 1930s in an attempt to understand the Great Depression. Keynes advocated increased government expenditures and lower taxes to stimulate demand and pull the global economy out of the Depression.\n",
            " 4 16              -0.39781 The approach is based on a theory of justice that considers crime and wrongdoing to be an offense against an individual or community, rather than the State. Restorative justice that fosters dialogue between victim and offender has shown the highest rates of victim satisfaction and offender accountability.\n",
            " 5 453             -0.42591 Perhaps the most influential person from a global perspective is David Hume (philosopher) or Adam Smith (economist). From the perspective of Scottish history, Robert the Bruce or William Wallace have been very influential.In modern times, it is probably Alex Salmond.If you want to make a suggestion about who should be in list (or who shouldnât) you can leave a comment below. 1.rom the perspective of Scottish history, Robert the Bruce or William Wallace have been very influential. In modern times, it is probably Alex Salmond. If you want to make a suggestion about who should be in list (or who shouldnât) you can leave a comment below. 1.\n",
            " 6 239             -0.47696 John Maynard Keynes, 1st Baron Keynes, CB (5 June 1883 â 21 April 1946) was a British economist. His ideas, called Keynesian economics, had a big impact on modern economic and political theory. His ideas also had a big impact on many governments' tax and economic policies.\n",
            " 7 745             -0.48573 Indeed, in Europe and across the pond in the United States, many learning of the archduke's death were less concerned with the drumbeats of war than the question of Austrian succession. The Washington Post, for example, published this largely fluffy piece on the royal who became the heir presumptive:\n",
            " 8 6               -0.49433 Nor will it attempt to substitute for the extraordinarily rich literature on the atomic bombs and the end of World War II. This collection does not attempt to document the origins and development of the Manhattan Project.\n",
            " 9 244             -0.56043 British economist John Maynard Keynes spearheaded a revolution in economic thinking that overturned the then-prevailing idea that free markets would automatically provide full employmentâthat is, that everyone who wanted a job would have one as long as workers were flexible in their wage demands (see box).\n",
            "10 243             -0.61034 John Maynard Keynes, 1st Baron Keynes, CB, FBA (/ËkeÉªnz/ KAYNZ; 5 June 1883 â 21 April 1946), was a British economist whose ideas fundamentally changed the theory and practice of modern macroeconomics and the economic policies of governments.\n"
          ]
        }
      ],
      "source": [
        "# Reranking\n",
        "# Define a query\n",
        "query = Query('who proposed the geocentric theory')\n",
        "# Load subset of corpus\n",
        "corpus = load_dataset(\"BeIR/msmarco\", 'corpus', split='corpus[0:1000]')\n",
        "# Define a document where the answer is\n",
        "passages = [['7744105',\n",
        "             'For Earth-centered it was  Geocentric Theory proposed by greeks under the guidance of Ptolemy and Sun-centered was Heliocentric theory proposed by Nicolas Copernicus in 16th century A.D. In short, Your Answers are: 1st blank - Geo-Centric Theory. 2nd blank - Heliocentric Theory.']]\n",
        "\n",
        "# Dataset of Text from corpus\n",
        "texts = [Text(p['text'], {'docid': p['_id']}, 0) for p in tqdm(corpus)]\n",
        "# Add corpus with high relevance\n",
        "texts.extend([Text(p[1], {'docid': p[0]}, 0) for p in passages])\n",
        "# Load trained model\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"./rerank\", device_map=\"auto\")\n",
        "\n",
        "tokenizer = T5BatchTokenizer(\n",
        "        AutoTokenizer.from_pretrained(\"google/flan-t5-base\", use_fast=False),\n",
        "        batch_size=2)\n",
        "# Objective tokens for the msmarco-10k dataset\n",
        "token_false = '▁false'\n",
        "token_true = '▁true'\n",
        "# Define reranker\n",
        "reranker = MonoT5('castorini/monot5-base-msmarco-10k',model=model, tokenizer=tokenizer, token_false = token_false, token_true = token_true)\n",
        "# Inference\n",
        "reranked = reranker.rerank(query, texts)\n",
        "\n",
        "# Print out reranked results:\n",
        "for i in range(0, 10):\n",
        "    print(f'{i + 1:2} {reranked[i].metadata[\"docid\"]:15} {reranked[i].score:.5f} {reranked[i].text}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "CrTOvyv5Rdvq",
      "metadata": {
        "id": "CrTOvyv5Rdvq"
      },
      "source": [
        "We can see that the correct document is first in the ranking but the others aren't separated enough, I ran a more formal benchmark according to https://github.com/castorini/pygaggle/blob/master/docs/experiments-msmarco-document.md.\n",
        "\n",
        "\n",
        "| metric | base | ours |\n",
        "| --- | --- | --- |\n",
        "| precision@1 | 0.2 | 0.08 |\n",
        "| recall@3 | 0.56 | 0.36 |\n",
        "| recall@50 | 0.84 | 0.76 |\n",
        "| recall@1000 | 0.88 | 0.88 |\n",
        "| mrr | 0.38882 | 0.24596 |\n",
        "| mrr@10 | 0.38271 | 0.23683 |\n",
        "\n",
        "From this we can see that on the entire corpus the new trained model has difficulty to find the correct best document but because the Mean Reciprocal Rank (MRR) is not too different it means that it's still in the top ranked documents.\n",
        "These results are actually very good, our model is trained only for 1 epoch and batch size 2 meanwhile the paper model is trained for 10 epochs and batch size 128.\n",
        "If we check the ablation study on the MRR with respect to the number of epochs we see that our model achieves perfomance comparable to T5-3B while having 10x times lesser parameters!\n",
        "This shows how much better is the new flan-t5 model compared to the previous one and that the implementation is correct.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "gen",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "db149d61c31d2624edf22e56b5a6b9ce4d73e6cd15e7216ac03fc615f0c4a733"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
